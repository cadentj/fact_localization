{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/u/caden/.conda/envs/autointerp/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "\n",
    "model = LanguageModel(\"openai-community/gpt2-xl\", device_map='auto', dispatch=True)\n",
    "tok = model.tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "string = '<|endoftext|>The Space Needle is in the city of'\n",
    "\n",
    "target = tok.encode(' Seattle', return_tensors='pt')[0][0]\n",
    "\n",
    "noise = torch.randn(1,4,1600) * 3 * 0.044414032250642776\n",
    "\n",
    "\n",
    "with model.trace(string):\n",
    "\n",
    "    clean_state = model.transformer.h[17].output[0].save()\n",
    "\n",
    "    clean_logits = model.lm_head.output.softmax(-1)[:,-1,target].save()\n",
    "    \n",
    "with model.trace(string):\n",
    "\n",
    "    model.transformer.wte.output[:,[1,2,3,4]] = noise\n",
    "\n",
    "    model.transformer.h[17].output[0][:,4,:] = clean_state[:,4,:]\n",
    "\n",
    "    restored_logits = model.lm_head.output.softmax(-1)[:,-1,target].save()\n",
    "\n",
    "print(clean_logits, restored_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLEAN:  tensor([0.9799], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "CORR:  tensor([0.0180], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "RESTORED AT 0:  tensor([0.0180], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "RESTORED AT 1:  tensor([0.0196], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "RESTORED AT 2:  tensor([0.0183], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "RESTORED AT 3:  tensor([0.0194], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "RESTORED AT 4:  tensor([0.1000], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "RESTORED AT 5:  tensor([0.0189], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "RESTORED AT 6:  tensor([0.0184], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "RESTORED AT 7:  tensor([0.0168], device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "string = '<|endoftext|>The Space Needle is in downtown'\n",
    "\n",
    "target = tok.encode(' Seattle', return_tensors='pt')[0][0]\n",
    "\n",
    "noise = torch.randn(1,4,1600) * 3 * 0.044414032250642776\n",
    "\n",
    "clean_states = {}\n",
    "\n",
    "# Sliding window of 5\n",
    "r = range(15,20)\n",
    "\n",
    "with model.trace(string):\n",
    "\n",
    "    for i in r:\n",
    "        clean_states[i] = model.transformer.h[i].mlp.act.output.save()\n",
    "\n",
    "    clean_logits = model.lm_head.output.softmax(-1)[:,-1,target].save()\n",
    "\n",
    "print(\"CLEAN: \", clean_logits)\n",
    "\n",
    "\n",
    "with model.trace(string):\n",
    "\n",
    "    model.transformer.wte.output[:,[1,2,3,4]] += noise\n",
    "\n",
    "    corr_logits = model.lm_head.output.softmax(-1)[:,-1,target].save()\n",
    "\n",
    "print(\"CORR: \", corr_logits)\n",
    "\n",
    "for token in range(8):\n",
    "    with model.trace(string):\n",
    "\n",
    "        model.transformer.wte.output[:,[1,2,3,4]] += noise\n",
    "\n",
    "        for i in r:\n",
    "            model.transformer.h[i].mlp.act.output[:,token,:] = clean_states[i][:,token,:]\n",
    "\n",
    "        restored_logits = model.lm_head.output.softmax(-1)[:,-1,target].save()\n",
    "\n",
    "    print(f\"RESTORED AT {token}: \", restored_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "STDEV = 0.044414032250642776\n",
    "\n",
    "results = torch.zeros((48, 10))\n",
    "\n",
    "string = '<|endoftext|>The Space Needle is in the city of'\n",
    "prompt = tok.encode(string)\n",
    "target = tok.encode(' Seattle', return_tensors='pt')[0][0]\n",
    "subject_tokens = [1,2,3,4]\n",
    "\n",
    "print(len(prompt))\n",
    "\n",
    "for _ in tqdm(range(10)):\n",
    "\n",
    "    noise = torch.randn(1,4,1600) * 3 * STDEV\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        clean_states = {}\n",
    "\n",
    "        with model.trace(string):\n",
    "\n",
    "            for layer in range(48):\n",
    "                clean_states[layer] = model.transformer.h[layer].mlp.output.cpu().save()\n",
    "\n",
    "            clean_logits = model.lm_head.output.softmax(-1)[:,-1,target].save()\n",
    "\n",
    "        with model.trace(string):\n",
    "\n",
    "            model.transformer.wte.output[:,subject_tokens] = noise\n",
    "\n",
    "            corr_logits = model.lm_head.output.softmax(-1)[:,-1,target].save()\n",
    "\n",
    "        print('clean logits:', clean_logits.value.item())\n",
    "        print('corrupted logits:', corr_logits.value.item())\n",
    "\n",
    "        for i in range(48):\n",
    "\n",
    "            for _tok in range(10):\n",
    "                \n",
    "                with model.trace(string):\n",
    "\n",
    "                    model.transformer.wte.output[:,subject_tokens] = noise\n",
    "\n",
    "                    model.transformer.h[i].mlp.output[:, _tok, :] = clean_states[i][:,_tok,:]\n",
    "\n",
    "                    restored_logits = model.lm_head.output.softmax(-1)[:,-1,target].save()\n",
    "\n",
    "                    diff = restored_logits - corr_logits \n",
    "\n",
    "                    diff.save()\n",
    "\n",
    "                results[i, _tok] += diff.value.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "\n",
    "\n",
    "test = uniform_filter1d(results.numpy().T, size=5, axis=1, mode='reflect')\n",
    "\n",
    "def plot_trace(results, str_tokens):\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    cax = ax.imshow(results, cmap=\"Purples\", aspect=\"auto\")\n",
    "    fig.colorbar(cax, ax=ax, orientation=\"vertical\")\n",
    "    # ax.set_yticklabels(str_tokens)\n",
    "    ax.set_xlabel(\"single restored layer within GPT-2-XL\")\n",
    "\n",
    "plot_trace(test, None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autointerp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
